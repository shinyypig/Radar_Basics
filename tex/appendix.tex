\appendix
\chapter{基础知识}


\section{符号规定}

工欲善其事必先利其器, 符号书写规范是一切沟通学习的前提。本书中会使用到大量矩阵和向量, 相关符号书写规则如下：
\begin{enumerate}
    \item 标量：小写字母, 如$a,b,c$或者$\alpha, \beta, l$.
    \item 向量：加粗小写字母, 如$\bm{x},\bm{y},\bm{z}$.
    \item 矩阵：加粗大写字母, 如$\mathbf{A},\mathbf{B},\mathbf{C}$.
\end{enumerate}


此外, 可以使用如下形式来表明向量或矩阵的维度：
\begin{enumerate}
    \item 向量：$\bm{x} \in \mathbb{R}^{n \times 1}$ 或 $\bm{x} \in \mathbb{R}^{n}$ 表示\( n \times 1 \)的列向量.
    \item 矩阵：$\mathbf{A} \in \mathbb{R}^{m \times n}$表示 $m \times n$ 的矩阵.
\end{enumerate}
其中, $\mathbb{R}$表示实数域, 而复数域则表示为$\mathbb{C}$.

需要注意的是, 对于向量和矩阵中的元素, 按照书写规则, 也应当用小写字母表示. 比如, 对于向量$\bm{x}$, 其第$i$个元素可以表示为$x_i$, 而对于矩阵$\mathbf{A}$, 其第$i$行第$j$列的元素可以表示为$a_{ij}$. 因此, \( \bm{x}_i \) 或 \( \mathbf{A}_i \)这样的符号, 代表的是第\( i \)个向量或矩阵.

\section{矩阵函数}

\section{卷积与相关的矩阵表示}\label{apx.conv-corr-mat}

给定两个向量 \( \bm{x} \in \mathbb{C}^{M \times 1} \) 和 \( \bm{p} \in \mathbb{C}^{N \times 1} \)（不妨假设 \( M \geq N \)），其卷积（Convolution）定义为
\begin{equation}
    y_k = \sum_{m=1}^{M} x_{k-m+1} p_m,
    \quad k = 1,2,\ldots, M+N-1,
\end{equation}
其中，当 \(k-m+1\) 超出向量 \(\bm{x}\) 的有效索引范围时，约定 \(x_{k-m+1}=0\)。由此得到的卷积向量长度为 \(M+N-1\)，通常称为\emph{完全卷积}（full convolution）。在某些场景下，为避免边界零填充，也常采用\emph{有效卷积}（valid convolution），即仅在索引完全重叠的区间内进行计算：
\begin{equation}
    y_{k-M+1} = \sum_{m=1}^{N} x_{k-m+1} p_m,
    \quad k = M, M+1, \ldots, N,
\end{equation}
其结果向量长度为 \(N-M+1\)，仅对应于所有索引均有效的部分。

上述两种卷积均可写为矩阵形式
\begin{equation}
    \bm{y} = \mathbf{X}\bm{p},
\end{equation}
其中完全卷积对应的矩阵为
\[
    \mathbf{X} =
    \begin{bmatrix}
        x_1    & 0      & \cdots & 0      \\
        x_2    & x_1    & \ddots & \vdots \\
        \vdots & x_2    & \ddots & 0      \\
        x_M    & \vdots & \ddots & x_1    \\
        0      & x_M    & \ddots & x_2    \\
        \vdots & \ddots & \ddots & \vdots \\
        0      & \cdots & 0      & x_M
    \end{bmatrix}
    \in \mathbb{C}^{(M+N-1)\times N},
\]
而有效卷积对应的矩阵为
\[
    \mathbf{X} =
    \begin{bmatrix}
        x_N     & x_{N-1} & \cdots & x_1       \\
        x_{N+1} & x_N     & \cdots & x_2       \\
        \vdots  & \vdots  & \ddots & \vdots    \\
        x_{M}   & x_{M-1} & \cdots & x_{M-N+1}
    \end{bmatrix}
    \in \mathbb{C}^{(M-N+1) \times N}.
\]
这种``每条主对角线元素均相同''的矩阵称为Toeplitz 矩阵。与卷积不同，相关（correlation）运算定义为
\begin{equation}
    y_k = \sum_{m=1}^{M} \overline{x}_{k+m-1} p_m,
    \quad k = 1,2,\ldots, M+N-1.
\end{equation}
其矩阵形式可写为
\begin{equation}
    \bm{y} = \mathbf{X}^{\mathrm{H}} \bm{p},
\end{equation}
其中
\[
    \mathbf{X} =
    \begin{bmatrix}
        0      & \cdots  & 0       & x_1     & x_2     & \cdots  & x_M    \\
        0      & \iddots & x_1     & x_2     & \cdots  & x_M     & 0      \\
        \vdots & \iddots & \iddots & \iddots & \iddots & \iddots & \vdots \\
        x_1    & x_2     & \cdots  & x_M     & 0       & \cdots  & 0
    \end{bmatrix}
    \in \mathbb{C}^{N \times (M+N-1)}.
\]
若不进行边界零填充，则相关矩阵可写为
\[
    \mathbf{X} =
    \begin{bmatrix}
        x_1    & x_2     & \cdots & x_{M-N+1} \\
        x_2    & x_3     & \cdots & x_{M-N+2} \\
        \vdots & \vdots  & \ddots & \vdots    \\
        x_N    & x_{N+1} & \cdots & x_M
    \end{bmatrix}
    \in \mathbb{C}^{N \times (M-N+1)}.
\]
这种``每条反对角线元素均相同''的矩阵称为 Hankel 矩阵。

更进一步，卷积不仅可以写成\( \bm{y} = \mathbf{X} \bm{p} \)的形式，还可以写成
\[
    \bm{y} = \mathbf{P} \bm{x}.
\]
以有效卷积为例，其对应的矩阵为
\[
    \mathbf{P} =
    \begin{bmatrix}
        p_N    & p_{N-1} & \cdots & p_1    & 0       & \cdots & 0      \\
        0      & p_N     & \cdots & p_2    & p_1     & \cdots & 0      \\
        \vdots & \vdots  & \ddots & \ddots & \ddots  & \ddots & \vdots \\
        0      & 0       & \cdots & p_N    & p_{N-1} & \cdots & p_1
    \end{bmatrix}
    \in \mathbb{C}^{(M-N+1) \times M},
\]
类似地，相关也可以写成
\[
    \bm{y} = \mathbf{P}^{\mathrm{H}} \bm{x},
\]
其中
\[
    \mathbf{P} =
    \begin{bmatrix}
        p_1    & 0       & \cdots & 0      \\
        p_2    & p_1     & \cdots & 0      \\
        \vdots & \vdots  & \ddots & \vdots \\
        p_N    & p_{N-1} & \ddots & p_1    \\
        0      & p_N     & \ddots & p_2    \\
        \vdots & \vdots  & \ddots & \vdots \\
        0      & 0       & \cdots & p_N
    \end{bmatrix}
    \in \mathbb{C}^{M \times (M+N-1)}.
\]
这两种矩阵形式完全等价, 灵活地使用可以简化很多推导过程。

\section{线性规划}


\section{拉格朗日乘数法}\label{apx.lagrange-multiplier}
对于等式约束的优化问题：
\[
    \begin{cases}
        \min          & f(\bm{x})           \\
        \mathrm{s.t.} & h(\bm{x}) = \bm{0}.
    \end{cases}
\]
其取得极值的必要条件如下：
\begin{enumerate}
    \item \( h(\bm{x}) = 0 \)
    \item \( \frac{\partial f(\bm{x})}{\partial \bm{x}} + \lambda \frac{\partial h(\bm{x})}{\partial \bm{x}} = 0 \)
\end{enumerate}
其中\( \lambda \geq 0 \).

该如何理解上述两个条件呢？对于条件1，很显然它必须成立，因为该条件本身就是约束。对于条件2，则可以分为两种情况。情况1：该目标函数取得极值时，目标函数的导数并不为0 。此时，条件2表明\( \frac{\partial f(\bm{x})}{\partial \bm{x}} = -\lambda \frac{\partial h(\bm{x})}{\partial \bm{x}}  \)，也就是说目标函数梯度下降的方向与等式约束对应的函数的梯度方向刚好反向。这意味着想要进一步降低目标函数值，必然会影响等式约束的成立，因此，此时函数取得了极值。情况2：设\( \bm{x}^* \)处目标取得极值，并且\( h(\bm{x}^*) = 0\)，也就是说目标函数取得极值时，等式约束刚好满足。此时\( \frac{\partial f(\bm{x})}{\partial \bm{x}} = \bm{0} \)，显然当\( \lambda = 0 \)时，条件2是成立的。

根据如上观察，我们可以归纳出拉格朗日乘数法，其步骤如下：
\begin{enumerate}
    \item 构建拉格朗日函数
          \[
              \mathcal{L}(\bm{x}, \lambda) = f(\bm{x}) + \lambda h(\bm{x}).
          \]
    \item 对\( \bm{x} \)和\( \lambda \)分别求导，并令其为0，得到方程组
          \[
              \begin{cases}
                  \frac{\partial f(\bm{x})}{\partial \bm{x}} + \lambda \frac{\partial h(\bm{x})}{\partial \bm{x}} = 0 \\
                  h(\bm{x}) = 0
              \end{cases}.
          \]
    \item 求解上述方程组，得到\( (\bm{x}^*,\lambda^*)\)。
\end{enumerate}

\begin{example}
    设有如下的优化问题
    \[
        \begin{cases}
            \min          & f(\bm{x}) = x_1^2 + x_1x_2 +x_2^2 \\
            \mathrm{s.t.} & h(\bm{x}) = x_1^2 + x_2^2 - 1 = 0
        \end{cases}.
    \]
    请给出该问题的拉格朗日函数，并求解最优解。
\end{example}
\begin{solution}
    令向量\( \bm{x} = \begin{bsmallmatrix} x_1 \\ x_2 \end{bsmallmatrix} \)，则目标函数和约束函数可以写成如下的形式
    \[
        f(\bm{x}) = \bm{x}^{\mathrm{T}} \mathbf{R} \bm{x}, \quad h(\bm{x}) = \bm{x}^{\mathrm{T}} \bm{x} - 1,
    \]
    其中\( \mathbf{R} = \begin{bsmallmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & 1 \end{bsmallmatrix} \)。对应的拉格朗日函数为
    \[
        \mathcal{L}(\bm{x}, \lambda) = \bm{x}^{\mathrm{T}} \mathbf{R} \bm{x} + \lambda (\bm{x}^{\mathrm{T}} \bm{x} - 1).
    \]
    对\( \bm{x} \)和\( \lambda \)分别求导，并令其为0，得到方程组
    \[
        \begin{cases}
            \frac{\partial f(\bm{x})}{\partial \bm{x}} + \lambda \frac{\partial h(\bm{x})}{\partial \bm{x}} = 0 \\
            h(\bm{x}) = 0
        \end{cases} \Rightarrow
        \begin{cases}
            2\mathbf{R} \bm{x} + 2\lambda \bm{x} = 0 \\
            \bm{x}^{\mathrm{T}} \bm{x} - 1 = 0
        \end{cases}.
    \]
    不难发现\( \mathbf{R} \bm{x} = -\lambda \bm{x} \)，即\( \bm{x} \)是\( \mathbf{R} \)的特征向量，对应的特征值为\( -\lambda \)。而矩阵\( \mathbf{R} \)有如下特征分解：
    \[
        \mathbf{R} = \mathbf{U} \bm{\Sigma} \mathbf{U}^{\mathrm{T}} = \begin{bmatrix}
            \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}  \\
            \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}
        \end{bmatrix} \begin{bmatrix}
            \frac{3}{2} & 0           \\
            0           & \frac{1}{2}
        \end{bmatrix} \begin{bmatrix}
            \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}  \\
            \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}
        \end{bmatrix}.
    \]
    显然，其最小特征值为\( \frac{1}{2} \)，对应的模长为1的特征向量为\( \bm{x} = \pm \begin{bmatrix} \frac{\sqrt{2}}{2} \\ -\frac{\sqrt{2}}{2} \end{bmatrix} \)。

    \begin{figure}[htb!]
        \centering
        \begin{subfigure}{.4\textwidth}
            \centering
            \input{img/basic/kkt1.tikz}
            \caption{不满足极值条件}
            \label{fig_basic_kkt_eg_1}
        \end{subfigure}
        \begin{subfigure}{.4\textwidth}
            \centering
            \input{img/basic/kkt2.tikz}
            \caption{满足极值条件}
            \label{fig_basic_kkt_eg_2}
        \end{subfigure}
        \caption{等式约束的优化问题示例}
        \label{fig_basic_kkt_eg}
    \end{figure}

    从\cref{fig_basic_kkt_eg_1}可以看到，此时的点并不满足极值条件，目标函数的梯度（蓝色箭头）和约束函数的梯度（红色箭头）并不共线，这意味着沿着黄色箭头的方向，目标函数仍然是可以继续下降的，因此该点并不是极值点。而在极值点处，如\cref{fig_basic_kkt_eg_2}所示，目标函数的梯度和约束函数的梯度是共线的，这意味着在该点处，想要进一步降低目标函数值，必然会影响等式约束的成立，因此该点是极值点。
\end{solution}

\section{分块矩阵求逆公式}
\begin{theorem}\label{thm:block-matrix-inverse}
    给定分块矩阵
    \[
        \begin{bmatrix}
            \mathbf{A} & \mathbf{B} \\
            \mathbf{C} & \mathbf{D}
        \end{bmatrix},
    \]
    若\( \mathbf{A} \)和\( \mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B} \)均为可逆矩阵，则有
    \[
        \begin{bmatrix}
            \mathbf{A} & \mathbf{B} \\
            \mathbf{C} & \mathbf{D}
        \end{bmatrix}^{-1} = \begin{bmatrix}
            \mathbf{A}^{-1} + \mathbf{A}^{-1}\mathbf{B}(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1} & -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1} \\
            -(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}                                           & (\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}
        \end{bmatrix}.
    \]
\end{theorem}
\begin{proof}
    不妨假设对应的逆矩阵为
    \[
        \mathbf{M}^{-1} = \begin{bmatrix}
            \mathbf{E} & \mathbf{F} \\
            \mathbf{G} & \mathbf{H}
        \end{bmatrix}.
    \]
    则有
    \[
        \begin{bmatrix}
            \mathbf{A} & \mathbf{B} \\
            \mathbf{C} & \mathbf{D}
        \end{bmatrix}
        \begin{bmatrix}
            \mathbf{E} & \mathbf{F} \\
            \mathbf{G} & \mathbf{H}
        \end{bmatrix}
        = \begin{bmatrix}
            \mathbf{I} & \mathbf{0} \\
            \mathbf{0} & \mathbf{I}
        \end{bmatrix}.
    \]
    逐块展开，可得
    \[
        \begin{cases}
            \mathbf{A}\mathbf{E} + \mathbf{B}\mathbf{G} = \mathbf{I}, \\
            \mathbf{A}\mathbf{F} + \mathbf{B}\mathbf{H} = \mathbf{0}, \\
            \mathbf{C}\mathbf{E} + \mathbf{D}\mathbf{G} = \mathbf{0}, \\
            \mathbf{C}\mathbf{F} + \mathbf{D}\mathbf{H} = \mathbf{I}.
        \end{cases}
    \]

    根据第一式，可得
    \[
        \mathbf{E} = \mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{B}\mathbf{G},
    \]
    将其带入第三式，可得
    \[
        \mathbf{C}(\mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{B}\mathbf{G}) + \mathbf{D}\mathbf{G} = \mathbf{0},
    \]
    即
    \[
        (\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})\mathbf{G} = -\mathbf{C}\mathbf{A}^{-1},
    \]
    因此
    \begin{equation}
        \mathbf{G} = - (\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}.
    \end{equation}
    将其代入第一式，可得
    \begin{equation}
        \mathbf{E} = \mathbf{A}^{-1} + \mathbf{A}^{-1}\mathbf{B}(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}.
    \end{equation}

    类似地，根据第二式，可得
    \[
        \mathbf{F} = -\mathbf{A}^{-1}\mathbf{B}\mathbf{H},
    \]
    将其代入第四式，可得
    \[
        -\mathbf{C}\mathbf{A}^{-1}\mathbf{B}\mathbf{H} + \mathbf{D}\mathbf{H} = \mathbf{I},
    \]
    即
    \begin{equation}
        \mathbf{H} = (\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}.
    \end{equation}
    将其代入第二式，可得
    \begin{equation}
        \mathbf{F} = -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}.
    \end{equation}
    综上，即可得到分块矩阵求逆公式.
\end{proof}

\begin{corollary}\label{cor:woodbury-matrix-identity}
    伍德伯里矩阵恒等式（Woodbury Matrix Identity）:
    \[
        (\mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1} = \mathbf{A}^{-1} + \mathbf{A}^{-1}\mathbf{B}(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}.
    \]
\end{corollary}
\begin{proof}
    对于分块矩阵
    \[
        \begin{bmatrix}
            \mathbf{A} & \mathbf{B} \\
            \mathbf{C} & \mathbf{D}
        \end{bmatrix},
    \]
    若\( \mathbf{D} \)和\( \mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C} \)也为可逆矩阵，则有
    \[
        \begin{bmatrix}
            \mathbf{A} & \mathbf{B} \\
            \mathbf{C} & \mathbf{D}
        \end{bmatrix}^{-1} = \begin{bmatrix}
            (\mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1}                           & -(\mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1}\mathbf{B}\mathbf{D}^{-1}                                           \\
            -\mathbf{D}^{-1}\mathbf{C}(\mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1} & \mathbf{D}^{-1} + \mathbf{D}^{-1}\mathbf{C}(\mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1}\mathbf{B}\mathbf{D}^{-1}
        \end{bmatrix}.
    \]
    该分块矩阵的逆与\cref{thm:block-matrix-inverse}中的结果是等价的，对比两者的左上角块，即可得到伍德伯里矩阵恒等式.
\end{proof}


\chapter{向量范数与矩阵范数}\label{apx.matrix-norm}
首先给出范数的概念：

\begin{definition}[范数]
    若$V$是数域上的线性空间，在其上定义了一个实值函数$\|\cdot\|:V \to \mathbb{R}$，
    满足：
    \begin{enumerate}
        \item 非负性： $\|\bm{x}\| \geq 0$, $\|\bm{x}\|=0 \Leftrightarrow \bm{x} =0$
        \item 齐次性： $\|\alpha \bm{x}\|=|\alpha|\|\bm{x}\|$
        \item 三角不等式： $\|\bm{x}+\bm{y}\| \leq \|\bm{x}\|+\|\bm{y}\|$
    \end{enumerate}
    其中\( \bm{x} \)和\( \bm{y} \)是$V$中的任意矢量，$\alpha$为数域上的任意标量. 则称$\|\cdot\|$为线性空间$V$上的\textbf{范数}. 特别地，当线性空间的元素为矩阵时，相应的范数也称为\textbf{矩阵范数}.
\end{definition}

接下来，我们给出常用的向量范数与矩阵范数.

\section{向量范数}
我们以欧氏空间为例，给出$n$维欧氏空间中元素的各种常用范数. 其中，下面用到的向量$\bm{x}=\begin{bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}^{\mathrm{T}}$, $\bm{y}=\begin{bmatrix} y_1 & y_2 & \cdots & y_n \end{bmatrix}^{\mathrm{T}}$均为$n$维欧氏空间中的
向量.

\begin{enumerate}[leftmargin=0em, listparindent=2em, itemindent=2em]
    \item $\ell_1$-范数：
          \[
              \|\bm{x}\|_1 = \sum_{i=1}^n |x_i|.
          \]
          由向量的\( \ell_1 \)-范数（简称1-范数）可以定义向量之间的\textbf{曼哈顿距离（Manhattan Distance）}
          \[
              d_{\bm{x}\bm{y}} = \|\bm{x} - \bm{y}\|_1 = \sum_{i=1}^n |x_i - y_i|.
          \]
          \cref{fig_apx_l1}给出了二维平面上到原点的曼哈顿距离为1的所有的点的几何结构.
          \begin{figure}[htb!]
              \centering
              \includegraphics[width=.35\textwidth]{img/apx/l1.tikz}
              \caption{在二维平面上到原点的曼哈顿距离为 1 的点集的几何结构}
              \label{fig_apx_l1}
          \end{figure}
    \item $\ell_2$-范数：
          \[
              \|\bm{x}\|_2 = \left(\sum_{i=1}^n x_i^2\right)^{\frac{1}{2}}.
          \]
          由向量的\( \ell_2 \)-范数（简称2-范数）定义可以定义向量之间的\textbf{欧氏距离（Euclidean Distance）}：
          \[
              d_{\bm{x}\bm{y}} = \|\bm{x} - \bm{y}\| = \left(\sum_{i=1}^{n}(x_i - y_i)^2\right)^{\frac{1}{2}}.
          \]
          \cref{fig_apx_l2}给出了二维平面上到原点的欧氏距离为1的所有的点的几何结构.
          \begin{figure}[htb!]
              \centering
              \includegraphics[width=.35\textwidth]{img/apx/l2.tikz}
              \caption{在二维平面上到原点的欧氏距离为1的点集的几何结构}
              \label{fig_apx_l2}
          \end{figure}
    \item $\ell_p$-范数：
          \[
              \|\bm{x}\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}.
          \]
          由向量的$\ell_p$-范数（简称\( p \)范数）可以定义向量之间的\textbf{明斯基距离（Minkowski Distance）}：
          \[
              \bm{d}_{xy} = \|\bm{x} - \bm{y}\|_p = \left(\sum_{i=1}^n |x_i - y_i|^p\right)^{\frac{1}{p}}.
          \]
          \cref{fig_apx_lp}分别给出了$p = 0.5$和$p = 4$时二维平面上到原点的明斯基距离为1的所有的点的几何结构.

          \begin{figure}[htb!]
              \centering
              \includegraphics[width=.35\textwidth]{img/apx/lp.tikz}
              \caption{在二维平面上到原点明斯基距离为1的点集的几何结构}
              \label{fig_apx_lp}
          \end{figure}
    \item \( \infty \)-范数：
          \[
              \| \bm{x} \|_{\infty} = \max_{1 \leq i \leq n} |x_i|.
          \]
          由向量的\( \infty \)-范数可以定义向量之间的\textbf{切比雪夫距离（Chebyshev Distance）}：
          \[
              d_{\bm{x}\bm{y}} = \| \bm{x} - \bm{y} \|_{\infty} = \max_{1 \leq i \leq n} |x_i - y_i|.
          \]
          \( \infty \)-范数可以看作是\( \ell_p \)-范数在\( p\) 趋于无穷时的极限情形，即有
          \[
              \|\bm{x}\|_{\infty} = \lim_{p \to \infty} \left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}.
          \]
          相应地，到原点的切比雪夫距离为1的图形也延续上面几个图形的规律，为正方形结构（\cref{fig_apx_linf}）.
          \begin{figure}[htb!]
              \centering
              \includegraphics[width=.35\textwidth]{img/apx/linf.tikz}
              \caption{在二维平面上到原点的切比雪夫距离为1的点集的几何结构}
              \label{fig_apx_linf}
          \end{figure}
    \item \( \ell_0 \)-范数：
          \[
              \| \bm{x} \|_0 = \sum_{i=1}^n \mathbb{I}(x_i),
          \]
          其中，\( \mathbb{I}(\cdot) \) 为指示函数，有如下表达式
          \[
              \mathbb{I}(x_i) = \begin{cases}
                  1, & x_i \neq 0 \\
                  0, & x_i = 0
              \end{cases}.
          \]
          简而言之，向量的\( \ell_0 \)-范数（简称0-范数）等于向量中非零元素的个数. \cref{fig_apx_l0}给出了二维平面上两个分量都不大于1且\( \ell_0 \)-范数为1的所有的点的几何结构. 需要注意的是，向量的\( \ell_0 \)-范数并不是普通意义的范数，因为它并不满足范数齐次性的性质.
          \begin{figure}[htb!]
              \centering
              \includegraphics[width=.35\textwidth]{img/apx/l0.tikz}
              \caption{二维平面上两个分量都不大于1且\( \ell_0 \)-范数为1的点集的几何结构}
              \label{fig_apx_l0}
          \end{figure}
\end{enumerate}

\section{矩阵范数}

常用的矩阵范数（其中\( \mathbf{A} \)为\( m \times n \)实矩阵）如下.

\begin{enumerate}[leftmargin=0em, listparindent=2em, itemindent=2em]
    \item 弗罗贝尼乌斯范数（Frobenius Norm）

          矩阵\( \mathbf{A} \)的弗罗贝尼乌斯范数（简称F-范数）是最常用的矩阵范数，公式如下
          \[
              \|\mathbf{A}\|_{\mathrm{F}} = \sqrt{\operatorname{tr}(\mathbf{A}^{\mathrm{T}}\mathbf{A})} = \left(\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2\right)^{\frac{1}{2}}.
          \]
          事实上，它相当于将矩阵\( \mathbf{A} \)展成向量后的$\ell_2$-范数，即 $\|\mathbf{A}\|_{\mathrm{F}} = \|\operatorname{vec}(\mathbf{A})\|_2$.
    \item $p$-范数：

          矩阵\( \mathbf{A} \)的$p$-范数定义为
          \[
              \|\mathbf{A}\|_p = \max_{\bm{x} \neq 0} \frac{\|\mathbf{A}\bm{x}\|_p}{\|\bm{x}\|_p},
          \]
          其中，$\|\bm{x}\|_p$ 是向量\( \bm{x} \)的 \( \ell_p \)-范数. 需要注意的是，矩阵\( \mathbf{A} \)的2-范数和它的F-范数一般情况下并不等价.
    \item 行和范数（Row-sum Norm）

          \[
              \|\mathbf{A}\|_{row} = \max_{1\leq i\leq m} \left\{\sum_{j=1}^n |a_{ij}|\right\}.
          \]

    \item 列和范数（Column-sum Norm）

          \[
              \|\mathbf{A}\|_{col} = \max_{1\leq j\leq n} \left\{\sum_{i=1}^m |a_{ij}|\right\}.
          \]

    \item 谱范数（Spectrum Norm）

          \[
              \|\mathbf{A}\|_{spec} = \sigma_{max} = \sqrt{\lambda_{max}},
          \]
          其中$\sigma_{max}$是矩阵\( \mathbf{A} \)的最大奇异值，即$\mathbf{A}^{\mathrm{T}}\mathbf{A}$的最大特征值$\lambda_{max}$的平方根.

    \item 马哈拉诺比斯范数（Mahalanobis Norm）

          \[
              \|\mathbf{A}\|_{\mathbf{\Omega}} = \sqrt{\operatorname{tr}(\mathbf{A}^{\mathrm{T}}\mathbf{\Omega} \mathbf{A})},
          \]
          其中$\mathbf{\Omega}$为正定矩阵.

    \item 核范数（Nuclear Norm）

          \[
              \|\mathbf{A}\|_* = \operatorname{tr}(\sqrt{\mathbf{A}^{\mathrm{T}}\mathbf{A}}).
          \]
          经过简单的验证可知，矩阵的核范数等于矩阵的所有奇异值之和.
\end{enumerate}


\chapter{张量乘法性质}

\begin{property}\label{prop:apx_kron_matmul}
    给定两个矩阵\( \mathbf{A} \in \mathbb{R}^{M \times K} \)和\( \mathbf{B} \in \mathbb{R}^{N \times K} \)，则矩阵乘法可以写成如下的k模积的形式
    \[
        \mathbf{A} \mathbf{B}^{\mathrm{T}} = \mathbf{I} \times_1 \mathbf{A} \times_2 \mathbf{B},
    \]
    其中，\( \mathbf{I} \)为一个\( K \times K \)的单位矩阵。
\end{property}
\begin{proof}
    由k模积的定义，有
    \[
        \begin{split}
            \left( \mathbf{I} \times_1 \mathbf{A} \times_2 \mathbf{B} \right)_{mn} & = \sum_{i_2=1}^K \sum_{i_1=1}^{K} (\mathbf{I})_{i_1 i_2} (\mathbf{A})_{mi_1} (\mathbf{B})_{ni_2} \\
                                                                                   & = \sum_{i=1}^K (\mathbf{A})_{mi} (\mathbf{B})_{ni} = (\mathbf{A} \mathbf{B}^{\mathrm{T}})_{mn}.
        \end{split}
    \]
    证明完毕。
\end{proof}

\begin{property}\label{prop:apx_kron_kprod}
    利用张量模k展开，可以将k模积写成矩阵乘法形式。给定张量\( \mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N} \)和矩阵\( \mathbf{U} \in \mathbb{R}^{J \times I_k} \)，记
    \[
        \mathcal{X} = \mathcal{T} \times_k \mathbf{U} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_{k-1} \times J \times I_{k+1} \times \cdots \times I_N}
    \]
    则\( \mathcal{X} \)的模\( k \)展开\( \mathbf{X}_k \in \mathbb{R}^{J \times I_1 \cdots I_{k-1} I_{k+1} \cdots I_N}\)有如下表达式
    \[
        \mathbf{X}_k = \mathbf{U} \mathbf{T}_k,
    \]
    其中\( \mathbf{T}_k \in \mathbb{R}^{I_k \times I_1 \cdots I_{k-1} I_{k+1} \cdots I_N} \)为张量\( \mathcal{T} \)的模\( k \)展开。
\end{property}
\begin{proof}
    令\( r =\rho(i_1,\ldots,i_{k-1},i_{k+1},\ldots,i_N)\)为索引编码，满足如下关系
    \[
        (\mathbf{T}_k)_{i_k r} = (\mathcal{T})_{i_1\cdots i_k\cdots i_N}.
    \]
    因此有
    \[
        (\mathbf{X}_k)_{jr}=(\mathcal{T}\times_k \mathbf{U})_{i_1\cdots j\cdots i_N}.
    \]
    进一步地，根据k模积的定义，有
    \[
        (\mathbf{X}_k)_{jr} = \sum_{i_k=1}^{I_k} (\mathcal{T})_{i_1\cdots i_k\cdots i_N} (\mathbf{U})_{ji_k}.
    \]
    另一方面，矩阵乘法给出
    \[
        (\mathbf{U}\mathbf{T}_k)_{j r} = \sum_{i_k=1}^{I_k} (\mathbf{U})_{j i_k} (\mathbf{T}_k)_{i_k r} = \sum_{i_k=1}^{I_k} (\mathcal{T})_{i_1\cdots i_k\cdots i_N} (\mathbf{U})_{ji_k}.
    \]
    两式右端逐项相等，故 $(\mathbf{X}_k)_{j r}=(\mathbf{U}\mathbf{T}_k)_{j r}$，即
    $\mathbf{X}_k=\mathbf{U}\mathbf{T}_k$。证毕。
\end{proof}


\begin{property}\label{prop:kronecker-kron}
    给定张量\( \mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N} \)，以及两个矩阵\( \mathbf{A} \in \mathbb{R}^{J \times I_a} \)和\( \mathbf{B} \in \mathbb{R}^{K \times I_b} \)，则有
    \[
        (\mathcal{T} \times_a \mathbf{A} \times_b \mathbf{B})_{ab} = (\mathbf{B} \otimes \mathbf{A}) \mathbf{T}_{ab},
    \]
    其中\( (\cdot)_{ab} \)表示张量的模\( a,b \)展开。
\end{property}
\begin{proof}
    记 \(\mathcal{Y}=\mathcal{T}\times_a \mathbf{A}\times_b \mathbf{B}\)。
    按模乘定义（记分量用小写 \(t_{\cdot},a_{\cdot},b_{\cdot}\)），对任意
    \(1\le j\le J,1\le k\le K\) 与其余 \(\bm{i}_{\neg a,b}\) 有
    \[
        y_{i_1 \dots i_{a-1} j i_{a+1} \dots i_{b-1} k i_{b+1} \dots i_N}
        =
        \sum_{r=1}^{I_a}\sum_{s=1}^{I_b}
        a_{j r} b_{k s}
        t_{i_1 \dots i_{a-1} r i_{a+1} \dots i_{b-1} s i_{b+1} \dots i_N}.
    \]
    将 \(\mathcal{Y}\)、\(\mathcal{T}\) 分别按模 \((a,b)\) 展开，记
    \(\mathbf{Y}_{ab}\in\mathbb{R}^{JK\times \prod_{n\neq a,b} I_n}\)、\(\mathbf{T}_{ab}\in\mathbb{R}^{I_aI_b\times \prod_{n\neq a,b} I_n}\)。
    取行编码
    \(\pi(j,k)=(k-1)J+j\)、\(\rho(r,s)=(s-1)I_a+r\)，列编码
    \(\gamma(\bm{i}_{\neg a,b})\) 为其余指标的字典序，于是
    \[
        [\mathbf{Y}_{ab}]_{\pi(j,k),\gamma(\bm{i}_{\neg a,b})}
        =
        y_{i_1 \dots i_{a-1} j i_{a+1} \dots i_{b-1} k i_{b+1} \dots i_N}.
    \]
    代入上式分量表达并重写为矩阵索引求和，得
    \[
        \begin{aligned}
            [\mathbf{Y}_{ab}]_{\pi(j,k),\gamma(\bm{i}_{\neg a,b})}
             & = \sum_{r=1}^{I_a}\sum_{s=1}^{I_b}
            a_{j r} b_{k s}
            t_{i_1 \dots i_{a-1} r i_{a+1} \dots i_{b-1} s i_{b+1} \dots i_N} \\
             & = \sum_{r=1}^{I_a}\sum_{s=1}^{I_b}
            \underbrace{b_{k s} a_{j r}}_{=[(\mathbf{B}\otimes\mathbf{A})]_{\pi(j,k),\rho(r,s)}}
            \underbrace{[\mathbf{T}_{ab}]_{\rho(r,s),\gamma(\bm{i}_{\neg a,b})}}_{\text{模 }(a,b)\text{ 展开}} .
        \end{aligned}
    \]
    上式对所有列索引 \(\gamma(\bm{i}_{\neg a,b})\) 同时成立，故有矩阵等式
    \[
        \mathbf{Y}_{ab} = (\mathbf{B}\otimes\mathbf{A})\mathbf{T}_{ab}.
    \]
    有限重求和可交换，证明完毕。证毕。
\end{proof}

\begin{corollary}\label{cor:kronecker-kron}
    给定张量\( \mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N} \)，以及\( N \)个矩阵\( \mathbf{X}_n \in \mathbb{R}^{J_n \times I_n} \)，则有
    \[
        (\mathcal{T} \times_1 \mathbf{X}_1 \times_2 \mathbf{X}_2 \times_3 \cdots \times_N \mathbf{X}_N)_{1\cdots N} = (\mathbf{X}_N \otimes \mathbf{X}_{N-1} \otimes \cdots \otimes \mathbf{X}_1) \mathbf{T}_{1\cdots N},
    \]
    其中\( (\cdot)_{1\cdots N} \)表示张量的模\( 1,2,\ldots,N \)展开，通常也被记作\( \operatorname{vec}( \mathcal{T} ) = \mathbf{T}_{1\cdots N} \)，即将张量展平为列向量。
\end{corollary}

\begin{corollary}\label{cor:kronecker-vec}
    对于克罗内克积，有如下性质
    \[
        \operatorname{vec}(\mathbf{A} \mathbf{X} \mathbf{B}) = (\mathbf{B}^{\mathrm{T}} \otimes \mathbf{A}) \operatorname{vec}(\mathbf{X}),
    \]
\end{corollary}
\begin{proof}
    注意到\( \mathbf{A} \mathbf{X} \mathbf{B} \)可以写为如下的k模积形式
    \[
        \mathbf{A} \mathbf{X} \mathbf{B} = \mathbf{X} \times_1 \mathbf{A} \times_2 \mathbf{B}^{\mathrm{T}}.
    \]
    因此，利用\cref{cor:kronecker-kron}，我们有
    \[
        \operatorname{vec}(\mathbf{A} \mathbf{X} \mathbf{B}) = \operatorname{vec}( \mathbf{X} \times_1 \mathbf{A} \times_2 \mathbf{B}^{\mathrm{T}}) = (\mathbf{B}^{\mathrm{T}} \otimes \mathbf{A}) \operatorname{vec}(\mathbf{X}).
    \]
\end{proof}

\begin{property}
    给定张量\( \mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N} \)，以及两个矩阵\( \mathbf{A} \in \mathbb{R}^{A \times I_a} \)和\( \mathbf{B} \in \mathbb{R}^{B \times I_b} \)，则有
    \[
        \mathcal{T} \times_a \mathbf{A} \times_b \mathbf{B} = \mathcal{T} \times_b \mathbf{B} \times_a \mathbf{A}, \quad a \neq b,
    \]
    即k模积是可交换的。
\end{property}
\begin{proof}
    令
    \[
        \mathcal{X} = \mathcal{T}\times_a \mathbf{A}\times_b \mathbf{B}
        \quad\text{与}\quad
        \mathcal{Y} = \mathcal{T}\times_b \mathbf{B}\times_a \mathbf{A}.
    \]
    这两个张量的大小均为
    $I_1\times\cdots\times I_{a-1}\times A\times I_{a+1}\times\cdots\times
        I_{b-1}\times B\times I_{b+1}\times\cdots\times I_N$。
    对任意索引
    \[
        (i_1,\ldots,i_{a-1},\alpha, i_{a+1},\ldots,i_{b-1},\beta, i_{b+1},\ldots,i_N),
    \]
    由定义得
    \begin{align*}
        x_{i_1\cdots i_{a-1}\alpha i_{a+1}\cdots i_{b-1}\beta i_{b+1}\cdots i_N}
         & = \sum_{i_a=1}^{I_a}\sum_{i_b=1}^{I_b}
        t_{i_1\cdots i_a\cdots i_b\cdots i_N}
        a_{\alpha i_a}b_{\beta i_b},              \\
        y_{i_1\cdots i_{a-1}\alpha i_{a+1}\cdots i_{b-1}\beta i_{b+1}\cdots i_N}
         & = \sum_{i_b=1}^{I_b}\sum_{i_a=1}^{I_a}
        t_{i_1\cdots i_a\cdots i_b\cdots i_N}
        b_{\beta i_b}a_{\alpha i_a}.
    \end{align*}
    交换求和次序不改变结果，得证。
\end{proof}

\begin{property}\label{prop:associative}
    给定张量\( \mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N} \)，以及两个矩阵\( \mathbf{A} \in \mathbb{R}^{J \times I_k} \)和\( \mathbf{B} \in \mathbb{R}^{K \times J} \)，则有
    \[
        \mathcal{T} \times_k \mathbf{A} \times_k \mathbf{B} = \mathcal{T} \times_k (\mathbf{B} \mathbf{A}).
    \]
\end{property}
\begin{proof}
    根据k模积的定义，有
    \begin{align*}
        \left(\mathcal{T}\times_k \mathbf{A}\times_k \mathbf{B}\right)_{i_1\cdots i_{k-1}j i_{k+1}\cdots i_N}
         & =\sum_{l=1}^{J}\left(\sum_{i_k=1}^{I_k}
        t_{i_1\cdots i_k\cdots i_N} a_{l i_k}\right)b_{jl}                                                    \\
         & =\sum_{i_k=1}^{I_k}t_{i_1\cdots i_k\cdots i_N}
        \left(\sum_{l=1}^{J}b_{jl}a_{l i_k}\right)                                                            \\
         & =\sum_{i_k=1}^{I_k}t_{i_1\cdots i_k\cdots i_N}(\mathbf{B} \mathbf{A})_{j i_k}                      \\
         & = \left(\mathcal{T}\times_k (\mathbf{B}\mathbf{A})\right)_{i_1\cdots i_{k-1} j i_{k+1}\cdots i_N}.
    \end{align*}
    证毕。
\end{proof}

\begin{property}\label{prop:diag-kron}
    给定\( N \)阶对角张量\( \mathcal{I}_K \in \mathbb{R}^{K \times K \times \cdots \times K} \)，且对角元素都为1。此外，还有\( N \)个矩阵
    \[
        \mathbf{X}_n = \begin{bmatrix}
            \bm{x}_{n1} & \bm{x}_{n2} & \cdots & \bm{x}_{nK}
        \end{bmatrix} \in \mathbb{R}^{I_n \times K},
    \]
    则如下等式成立
    \[
        \mathcal{I}_K \times_1 \mathbf{X}_1 \times_2 \mathbf{X}_2 \times_3 \cdots \times_N \mathbf{X}_N = \sum_{k=1}^K \bm{x}_{1k} \circ \bm{x}_{2k} \circ \cdots \circ \bm{x}_{Nk},
    \]
\end{property}
\begin{proof}
    根据k模积的定义，有
    \[
        \left( \mathcal{I}_K \times_1 \mathbf{X}_1 \times_2 \cdots \times_N \mathbf{X}_N \right)_{i_1 \cdots i_N} = \sum_{k_1=1}^{K}\cdots\sum_{k_N=1}^{K} (\mathcal{I}_K)_{k_1\cdots k_N} (\mathbf{X}_1)_{i_1 k_1} \cdots (\mathbf{X}_N)_{i_N k_N}.
    \]
    又因为当且仅当 $k_1=k_2=\cdots=k_N$ 时，$(\mathcal{I}_K)_{k_1\cdots k_N}=1$，否则为0，所以有
    \[
        \begin{split}
            \left( \mathcal{I}_K \times_1 \mathbf{X}_1 \times_2 \cdots \times_N \mathbf{X}_N \right)_{i_1 \cdots i_N} & = \sum_{k=1}^{K} (\mathbf{X}_1)_{i_1 k} \cdots (\mathbf{X}_N)_{i_N k}                      \\
                                                                                                                      & = \sum_{k=1}^{K} (\bm{x}_{1k})_{i_1} \cdots (\bm{x}_{Nk})_{i_N}                            \\
                                                                                                                      & = \sum_{k=1}^K \left( \bm{x}_{1k} \circ \cdots \circ \bm{x}_{Nk} \right)_{i_1 \cdots i_N}.
        \end{split}
    \]
    因此，有
    \[
        \mathcal{I}_K \times_1 \mathbf{X}_1 \times_2 \cdots \times_N \mathbf{X}_N = \sum_{k=1}^K \bm{x}_{1k} \circ \bm{x}_{2k} \circ \cdots \circ \bm{x}_{Nk}.
    \]
\end{proof}
